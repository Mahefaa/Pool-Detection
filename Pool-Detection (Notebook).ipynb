{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool-Detection\n",
    "\n",
    "The aim of this project is to provide a POC (Proof Of Concept) to prove the feasibility of **detecting pools from satellite images**.\n",
    "\n",
    "This task will be divided into two steps:\n",
    "\n",
    "- Create an image classification model (Convnet) to distinguish between images with pools and images without *no_pools*.\n",
    "- Then use this model to scan satellite images and find the position of potential pools on the image.\n",
    "\n",
    "***\n",
    "\n",
    "## Available Data\n",
    "\n",
    "```tree\n",
    "└───data\n",
    "    ├───train\n",
    "    │   ├───no_pools\n",
    "    │   └───pools\n",
    "    ├───validation\n",
    "    │   ├───no_pools\n",
    "    │   └───pools\n",
    "    └───zones\n",
    "```\n",
    "\n",
    "To create the image classifier, we dispose of around 1500 images of each class (pools, no_pools). This data are divided into **train** and **validation** sets. The **train** and **validation** set contain respectively around 1400 and 100 images of each class. Each image has a dimension of 50 x 50.\n",
    "\n",
    "Sample images:\n",
    "\n",
    "*pools* : ![alt text](./data/train/pools/img0.jpg)\n",
    "\n",
    "*no_pools* : ![alt text](./data/train/no_pools/img0.jpg)\n",
    "\n",
    "As for the satellite images on which the detection of *pools* will be performed, we have 30 of them in the **data/zones/** directory.\n",
    "\n",
    "\n",
    "Sample satellite image *(zone18.jpg)*:\n",
    "\n",
    "![alt text](data/zones/zone18.jpg)*zone18.jpg*\n",
    "\n",
    "- Size of the training data :\n",
    "    - *pools* : 1398 (50x50) images\n",
    "    - *no_pools*: 1325 (50x50) images\n",
    "- Size of the validation data:\n",
    "    - *pools*: 179 (50x50) images\n",
    "    - *no_pools*: 176 (50x50) images\n",
    "\n",
    "***\n",
    "\n",
    "## Installing the required python packages\n",
    "\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "The baseline model is a simple 3-layered Convnet. This model is a simple implementation of a *Convolutional Neural Network* and will be used as reference (in terms of performance) to the rest of the tested models.\n",
    "\n",
    "![alt text](README/PoolNetBaseline_3.png)*PoolNetBaseline Architecture*\n",
    "\n",
    "## Detection Mechanisms\n",
    "\n",
    "For the detection part, we divide the image into a grid of 50x50 patches which give us 512 candidate positions to test per satellite image (see image below). The constraint that was imposed for this project is to process one image in less then 10 seconds, but with our baseline model (which is a very simple convnet) we are able to provide predictions for 30 satellite images in around 35 seconds on a CPU (with only two cores).\n",
    "\n",
    "![alt text](README/decomp.png)*Satellite image decomposition*\n",
    "\n",
    "Another post-processing has been implemented to provide better predictions. Since we do not have any priors about the position of pools within the satellite image, we test for adjacency on the boxes that we've detected (horizontally or vertically) within the grid. This case is encountered when the pool image is divided between two patches, so we merge the boxes and recompute the probability for that specific patch (Exp: see image below.).\n",
    "\n",
    "![alt text](README/merging_adj.png)*Merging adjacent bounding boxes for better location prediction*\n",
    "***\n",
    "\n",
    "## Results\n",
    "\n",
    "### Quality of the classifier\n",
    "\n",
    "![alt text](README/acc_loss_history_3.png)*Train/Validation Loss/Accuracy*\n",
    "\n",
    "### Quality of the detection\n",
    "\n",
    "Sample Detection image *(zone18.jpg)*:\n",
    "\n",
    "![alt text](README/pooldetection_th%3D0.5_zone18.jpg)*Detection on a satellite image with a threshold of 0.5 (the default output of the **detect** class)*\n",
    "\n",
    "![alt text](README/pooldetection_th%3D0.75_zone18.jpg)*Detection on a satellite image with a threshold of 0.75 on the probability of each patch*\n",
    "\n",
    "Along with the image we provide a dictionary that contains all information relative to the position and probabilities of each bounding boxes (In this context we only keep the patches with a probability > 0.5, as the purpose of this project is to prove the feasibility of such detection.).\n",
    "\n",
    "Data relative to the detection image above *(zone18.jpg)*:\n",
    "\n",
    "**\"The reason we have only 6 bounding boxes in the detected image above is that we have a applied a filter that will only display patches with probability >= 0.75. This is done only for purpose of testing and visualization, by default the **detect** class will display all potential pool patches. (ie: probability >= 0.5)\"**\n",
    "\n",
    "```json\n",
    "\"./data/zones/zone18.jpg\": {\n",
    "        \"pos\": [\n",
    "            [250,100],\n",
    "            [1400,200],\n",
    "            [1100,250],\n",
    "            [900,500],\n",
    "            [850,550],\n",
    "            [1450,650],\n",
    "            [975,50]\n",
    "        ],\n",
    "        \"probas\": [\n",
    "            0.9920390248298645,\n",
    "            0.7856220006942749,\n",
    "            0.5162228345870972,\n",
    "            0.9985809922218323,\n",
    "            0.878549337387085,\n",
    "            0.9921371936798096,\n",
    "            0.9999746084213257\n",
    "        ],\n",
    "        \"nbPools\": 7\n",
    "}\n",
    "```\n",
    "\n",
    "Each tuple in the *pos* list represents the (x,y) coordinates of the top left corner of the bounding box that represents a potential pool(to get the center of the bounding box, we just add 25 to x and y as we use a 50x50 sliding window for the detection.). For better detection, we try to improve the predicted position of the bounding boxes by merging two adjacent boxes as sometimes a *pool* could be in two sliding window. This approach helps to make better detections and get closer to the real number of pools in that snapshot.\n",
    "\n",
    "### Heatmaps\n",
    "\n",
    "The use of the heatmap is primarily to understand the behaviour of the implemented model and helps make the detection of potential targets easier. We use the probabilities we predicted in the previous task then scale the probability matrix to the size of the input image so we can overlay the resulting heatmap (generated from the probability matrix) onto the image. We use a ```fading_agent``` which is a matrix that creates the fading effect on the heatmap, this is used to indicate the center of the detection as the potential position of the pool.\n",
    "\n",
    "![alt text](predictions/images/heatmaps/heatmap_zone18.jpg)*Example of a heatmap overlayed on satellite image*\n",
    "\n",
    "***\n",
    "\n",
    "## Going Further\n",
    "\n",
    "This work is considered a Proof of Concept to prove the feasibility of the task and is intended to be a baseline work that can be later improved to create a better detection model.\n",
    "\n",
    "In this section, I will propose three other approaches that require more data and more preparation:\n",
    "\n",
    "1. Mine for more data from satellite image APIs or online maps to create more robust data and create an extensive database that includes the position of pools on each image. Then use this data to train an Object Detection model. (exp: ```Fast R-CNN```)\n",
    "\n",
    "2. Use existing state of the art model for image classification like: ```Resnet```. But from the tests that I conducted during the preparation of this project, these model perform poorly (compared to simple ```Convnets```) on **low resolution** images. For this purpose, the use of a ```SRGAN (Super Resolution GAN)``` to upscale the training images to a size where we can use the aforementioned models without the degradation of performance can be a useful step better results. (A. Upscale the train images B. Train the model (transfer learning) on these images C. Scale the satellite images or use images with higher resolution D. Detect! = *same workflow*)\n",
    "\n",
    "3. There are plenty of research that have been done on low resolution image recognition that can be implemented for this case as we are dealing with very low resolution images (50 x 50). Since these models are optimized to handle this type of input, we can *expect* better results compared to simple ```Convnet```.\n",
    "\n",
    "### Credits\n",
    "- [[1] Keras SR-GAN: Enhancing low resolution images by applying deep networks with adversarial networks (Generative Adversarial Networks) \n",
    "to produce high resolutions images.](https://github.com/deepak112/Keras-SRGAN)\n",
    "- [Blog Keras for the necessary documentation on Keras](https://blog.keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "```console\n",
    "keras==2.2.4\n",
    "tensorflow==1.15.0\n",
    "numpy==1.16.5\n",
    "matplotlib==3.1.1\n",
    "seaborn==0.9.0\n",
    "pydot==1.4.1\n",
    "graphiz==2.38\n",
    "```\n",
    "**graphiz** and **pydot** are only necessary to the `plot_model` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a baseline Convnet for Pool Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load PoolNet.py\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Dense, Flatten\n",
    "from keras import applications\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "class PoolNetBaseline:\n",
    "    # this is a baseline, a simple convnet model used as\n",
    "    # a proof of concept\n",
    "    @staticmethod\n",
    "    def build(width:int, height:int, channels:int)->Sequential:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            inputShape = (channels, width, height)\n",
    "        else:\n",
    "            inputShape = (width, height, channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3,3), input_shape=inputShape))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (5,5))) # extra\n",
    "        model.add(Activation('relu')) # extra\n",
    "        model.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "        model.add(Conv2D(32, (3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (5,5))) # extra\n",
    "        model.add(Activation('relu')) # extra\n",
    "        model.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "        model.add(Conv2D(32, (3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (5,5))) # extra\n",
    "        model.add(Activation('relu')) # extra\n",
    "        model.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "        # at this stage the model outputs a 3D feature map of the image\n",
    "\n",
    "        model.add(Flatten()) # converts the 3D feature map into 1D feature vector\n",
    "        model.add(Dense(64))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.1)) # added to limit overfitting\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid')) # return a probability for the True class\n",
    "        # in quality of classification\n",
    "        # (Pool), forces the output to be in [0,1]\n",
    "        # print(model.summary())\n",
    "        # now return the neural network\n",
    "        return model\n",
    "\n",
    "# Leveraging pre-trained model from keras.applications\n",
    "# Performs poorly on low res images\n",
    "class PoolNetResnet:\n",
    "    @staticmethod\n",
    "    def build(width:int, height:int, channels:int)->Sequential:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            inputShape = (channels, width, height)\n",
    "        else:\n",
    "            inputShape = (width, height, channels)\n",
    "        inp = Input(shape=inputShape)\n",
    "        model = applications.vgg16.VGG16(include_top=False, pooling='avg', input_tensor=inp)\n",
    "        x = model.layers[-1].output\n",
    "\n",
    "        x = Dense(64)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x) # added to limit overfitting\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "        for i in range(len(model.layers)-2):\n",
    "            model.layers[i].trainable = False\n",
    "        # print(model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the data generator and the transformation to be done on input images for a more robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Train.py\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from PoolNet import PoolNetBaseline\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, model:Sequential, saveFile:str='best_weights_baseline.h5', *args):\n",
    "        self.CHECKPOINTPATH = './predictions/weights/'+saveFile\n",
    "        self.VALDIR = './data/validation'\n",
    "        self.TRAINDIR = './data/train'\n",
    "        self.IMGWIDTH, self.IMGHEIGHT = 50, 50\n",
    "        self.CHANNELS = 3\n",
    "        self.class1 = '/pools'\n",
    "        self.class2 = '/no_pools'\n",
    "\n",
    "        self.graphPath = './predictions/history/acc_loss_history.png'\n",
    "        self.batchSize = 16 # default values will be changed later\n",
    "        self.nbEpochs = 50 # default values will be changed later\n",
    "        self.nbValidation = len([1 for f in os.listdir(self.VALDIR+self.class1) \n",
    "            if os.path.isfile(os.path.join(self.VALDIR+self.class1, f))]) + \\\n",
    "            len([1 for f in os.listdir(self.VALDIR+self.class2) \n",
    "                if os.path.isfile(os.path.join(self.VALDIR+self.class2, f))]) \n",
    "        self.nbTrain = len([1 for f in os.listdir(self.TRAINDIR+self.class1) \n",
    "                if os.path.isfile(os.path.join(self.TRAINDIR, f))]) + \\\n",
    "            len([1 for f in os.listdir(self.TRAINDIR+self.class2) \n",
    "                if os.path.isfile(os.path.join(self.TRAINDIR+self.class2, f))])\n",
    "        \n",
    "        self.trainGenerator = None\n",
    "        self.valGenerator = None\n",
    "        self.modelHistory = None\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "    def dataAugmentation(self)->None:\n",
    "        # Data augmentation: Transformation to perform on input\n",
    "        # images to make the classifier more robust to noisier\n",
    "        # and lower quality input images that might be given during\n",
    "        # the testing phase\n",
    "\n",
    "        # Normal data augmentation\n",
    "        trainDataGen = ImageDataGenerator(\n",
    "            rotation_range=90,  #default value 180\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            shear_range=0.3,\n",
    "            rescale=1./255,\n",
    "            zoom_range=0.3, # default value 0.2\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        # Aggressive data augmentation\n",
    "        # trainDataGen = ImageDataGenerator(\n",
    "        #     rotation_range=180,\n",
    "        #     width_shift_range=0.5,\n",
    "        #     height_shift_range=0.5,\n",
    "        #     shear_range=0.2,\n",
    "        #     rescale=1./255,\n",
    "        #     zoom_range=0.3,\n",
    "        #     horizontal_flip=True,\n",
    "        #     fill_mode='nearest'\n",
    "        # )\n",
    "        # For the validation process the only transformation to make is\n",
    "        # the rescaling of the imag to avoid altering the input\n",
    "        valDataGen = ImageDataGenerator(\n",
    "            rescale=1./255\n",
    "        )\n",
    "\n",
    "        self.trainGenerator = trainDataGen.flow_from_directory(\n",
    "            self.TRAINDIR,\n",
    "            target_size=(self.IMGHEIGHT, self.IMGWIDTH),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        self.valGenerator = valDataGen.flow_from_directory(\n",
    "            self.VALDIR,\n",
    "            target_size=(self.IMGHEIGHT, self.IMGWIDTH),\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "    def trainModel(self, useAutoAugmentation:bool=True, trackPerf:bool=True)->None:\n",
    "        if useAutoAugmentation:\n",
    "            self.dataAugmentation()\n",
    "        # Keras Callbacks:\n",
    "        # EarlyStopping: Used to stop the training when there are no more gains\n",
    "        #   in terms of loss (decreasing) and accuracy (increasing)\n",
    "        # ModelCheckpoint: Used to save the weights of the best performing model\n",
    "        #   on the validation dataset\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(self.CHECKPOINTPATH, monitor='val_loss', save_best_only=True),\n",
    "            EarlyStopping(monitor='val_loss', patience=5)\n",
    "        ]\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer='sgd',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        self.modelHistory = self.model.fit_generator(\n",
    "            self.trainGenerator,\n",
    "            steps_per_epoch=self.nbTrain // self.batchSize,\n",
    "            epochs=self.nbEpochs,\n",
    "            validation_data=self.valGenerator,\n",
    "            validation_steps=self.nbValidation // self.batchSize,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        if trackPerf:\n",
    "            self.__trackModel__()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def __trackModel__(self)->None:\n",
    "        # Plot the evolution of the loss and accuracy for training and validation\n",
    "        # through the epochs\n",
    "        epochs = range(len(self.modelHistory.history['acc']))\n",
    "        _, ax1 = plt.subplots()\n",
    "        ax1.plot(epochs, self.modelHistory.history['loss'], label='train_loss', color='blue')\n",
    "        ax1.plot(epochs, self.modelHistory.history['val_loss'], label='val_loss', color='red')\n",
    "        ax1.set_xlabel(\"# Epochs\")\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(epochs, self.modelHistory.history['acc'], label='train_acc', color='green')\n",
    "        ax2.plot(epochs, self.modelHistory.history['val_acc'], label='val_acc', color='orange')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.title('Loss and Accuracy during the Learning Process')\n",
    "        plt.savefig(self.graphPath)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detection phase: applying the model on the satellite images, creating the bounding boxes, fine tuning the predictions and plotting the images with the bounding boxes and corresponding probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Detect.py\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, model:Sequential, zonesPath:str):\n",
    "        self.model = model\n",
    "        self.zonesPath = zonesPath\n",
    "        self.zones = [os.path.join(zonesPath, f) for f in os.listdir(zonesPath) \n",
    "            if os.path.isfile(os.path.join(zonesPath, f))]\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def removePrefix(self, s:str)->str:\n",
    "        if s.startswith(self.zonesPath):\n",
    "            return s[len(self.zonesPath):]\n",
    "        return s\n",
    "\n",
    "    def splitImage(self, img:list)->list:\n",
    "        # Splits an image of size `satHeight x satWidth` into a list of subimages of size = (imgHeight,imgWidth)\n",
    "        return [img[:, imgWidth*row:imgWidth*(row+1),imgHeight*col:imgHeight*(col+1),:]\n",
    "            for row in range(satHeight//imgHeight) for col in range(satWidth//imgWidth)]\n",
    "\n",
    "    def prepImages(self)->np.ndarray:\n",
    "        # Splits all the images into `imgHeight x imgWidth` subimage of size = (imgHeight,imgWidth)\n",
    "        # so we get one np.array for all satellite image to test (for easier and faster predictions\n",
    "        # of the probabilities)\n",
    "        imgs = []\n",
    "        for z in self.zones:\n",
    "            img = (img_to_array(load_img(z)) * (1./255))\n",
    "            img = img.reshape((1,) + img.shape)\n",
    "            imgs.extend(self.splitImage(img))\n",
    "        return np.array(imgs).reshape((len(imgs),imgHeight,imgWidth,channels))\n",
    "\n",
    "    def predictProba(self)->tuple:\n",
    "        x = self.prepImages()\n",
    "        probas = self.model.predict_proba(x, batch_size=batchSize).reshape(len(x))\n",
    "        probas = probas.reshape((len(self.zones), (satHeight//imgHeight)*(satWidth//imgWidth)))\n",
    "        return np.where(probas>=self.threshold, probas, 0.), dict(zip(self.zones, probas))\n",
    "\n",
    "    def getAdjacentBoxes(self, coords:list)->list:\n",
    "        # naive approach, this can be further optimized for high resolution\n",
    "        # images with a high number of detected instances \n",
    "        adj = []\n",
    "        for i in range(len(coords)):\n",
    "            for j in range(i+1,len(coords)):\n",
    "                q = self.adjacent(coords[i], coords[j])\n",
    "                if  q:\n",
    "                    adj.append([i,j,q])\n",
    "                else:\n",
    "                    continue\n",
    "        return adj\n",
    "\n",
    "    def adjacent(self, box1:tuple, box2:tuple)->bool:\n",
    "        if box1[1]==box2[1] and (box1[0]+50 == box2[0] or box1[0]-50 == box2[0]) or \\\n",
    "            box1[0]==box2[0] and (box1[1]+50 == box2[1] or box1[1]-50 == box2[1]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def cleanProba(self, probas:np.ndarray)->dict:\n",
    "        res = {}\n",
    "        # add a test to detect two adjacent boxes\n",
    "        # if y-adjacent center the boxes on the y-axis\n",
    "        # if x-adjacent center the boxes on the x-axis\n",
    "        for idx, p in enumerate(probas):\n",
    "            p = p.reshape(satHeight//imgHeight, satWidth//imgWidth)\n",
    "            x,y = np.nonzero(p)\n",
    "            res[self.zones[idx]] = {\n",
    "                \"pos\":list(zip(y*50,x*50)),\n",
    "                \"probas\":[ float(p[row][col]) for row,col in list(zip(x,y))],\n",
    "                \"adjacentBoxes\":self.getAdjacentBoxes(list(zip(y*50,x*50)))\n",
    "                # check for adjacent boxes for better detection\n",
    "            }\n",
    "            # find adjacent boxes\n",
    "            toRemove = {\n",
    "                \"pos\":[],\n",
    "                \"probas\":[]\n",
    "            }\n",
    "            predAgain = []\n",
    "            for bb in res[self.zones[idx]][\"adjacentBoxes\"]:\n",
    "                # get old values\n",
    "                b1 = res[self.zones[idx]][\"pos\"][bb[0]]\n",
    "                b2 = res[self.zones[idx]][\"pos\"][bb[1]]\n",
    "                # stage for for modifications and new probabilities\n",
    "                toRemove[\"pos\"].extend([bb[0],bb[1]])\n",
    "                toRemove[\"probas\"].extend([bb[0],bb[1]])\n",
    "                # newly better placed box\n",
    "                newBox = ((b1[0]+b2[0])//2, (b1[1]+b2[1])//2)\n",
    "                predAgain.append(newBox)\n",
    "            res[self.zones[idx]][\"probas\"] = [float(res[self.zones[idx]][\"probas\"][i])\n",
    "                for i in range(len(res[self.zones[idx]][\"probas\"])) if i not in set(toRemove[\"probas\"])]\n",
    "            res[self.zones[idx]][\"pos\"] = [(int(res[self.zones[idx]][\"pos\"][i][0]),int(res[self.zones[idx]][\"pos\"][i][1]))\n",
    "                for i in range(len(res[self.zones[idx]][\"pos\"])) if i not in set(toRemove[\"pos\"])]\n",
    "            # compute the probability for the new box\n",
    "            # needs to be optimized!!!\n",
    "            image = (img_to_array(load_img(self.zones[idx]))*1./255)\n",
    "            image = image.reshape((1,)+image.shape)\n",
    "            for h,w in predAgain:\n",
    "                x = image[:,w:w+50,h:h+50,:]\n",
    "                pr = self.model.predict_proba(x)[0][0]\n",
    "                if pr >= self.threshold:\n",
    "                    res[self.zones[idx]][\"pos\"].append((int(h),int(w)))\n",
    "                    res[self.zones[idx]][\"probas\"].append(float(pr))   \n",
    "            res[self.zones[idx]][\"nbPools\"] = len(res[self.zones[idx]][\"pos\"])\n",
    "            res[self.zones[idx]].pop(\"adjacentBoxes\",None)\n",
    "        self.results = res\n",
    "        return res\n",
    "\n",
    "    def drawBoxes(self)->None:\n",
    "        for img in self.results:\n",
    "            _, ax = plt.subplots(figsize=(20,10))\n",
    "            ax.imshow(load_img(img))\n",
    "            imgData = self.results[img]\n",
    "            # draw the bounding boxes and the probability for each\n",
    "            # classification\n",
    "            for coord, proba in zip(imgData[\"pos\"], imgData[\"probas\"]):\n",
    "                color = \"cyan\"\n",
    "                if proba >= 0.85: color = \"lime\"\n",
    "                if proba <0.75: color=\"crimson\"\n",
    "                # drawing the bounding boxes\n",
    "                rect = patches.Rectangle(coord,imgWidth, imgHeight,\n",
    "                    linewidth=2, edgecolor=color, facecolor=color, alpha=0.4)\n",
    "                ax.add_patch(rect)\n",
    "                # drawing the probabilities for each classification\n",
    "                if coord[1]+60 >= satHeight:\n",
    "                    xy = (coord[0],coord[1]-10)\n",
    "                else:\n",
    "                    xy = (coord[0],coord[1]+60)\n",
    "                \n",
    "                ax.annotate(\n",
    "                    s=\"prob:{:.3f}\".format(proba),\n",
    "                    xy=xy, color=color,weight=\"bold\", ha=\"center\", va=\"center\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.savefig(\"./predictions/images/pooldetection_th={}_{}\".format(self.threshold, self.removePrefix(img)))\n",
    "        return\n",
    "\n",
    "    def drawHeatmap(self, probaMap:list)->None:\n",
    "        # cmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=False)\n",
    "        cmap = sns.color_palette(\"Paired\")\n",
    "        # not the best implementation for it\n",
    "        def f(i:int,j:int)->float:\n",
    "            # fading function\n",
    "            fadingRate = 1./(20*np.sqrt(2)) # 24*np.sqrt(2) default value\n",
    "            # fadingRate = 0 # Debugging Value\n",
    "            return max([1 - fadingRate * (np.sqrt((24-i)**2+(24-j)**2)),0])\n",
    "        # the fadingAgent is used to create the fading effect on the heatmap\n",
    "        fadingAgent = np.array([[f(i,j) for i in range(50)] for j in range(50)])\n",
    "        for img in self.zones:\n",
    "            # Loading the image on which we will overlay the heatmap\n",
    "            _, ax = plt.subplots(figsize=(20,10))\n",
    "            image = load_img(img)\n",
    "            ax.imshow(image)\n",
    "            # Potential targets to mark the potential positions of pools\n",
    "            potTargets = [[],[]] # [xs:list, ys:list]\n",
    "            tmpMap = probaMap[img].reshape(satHeight//imgHeight, satWidth//imgWidth)\n",
    "            # scale up the probability matrix\n",
    "            newProbaMap = np.zeros((800,1600))\n",
    "            for i in range(satHeight//imgHeight):\n",
    "                for j in range(satWidth//imgWidth):\n",
    "                    newProbaMap[i*50:(i+1)*50, j*50:(j+1)*50] = tmpMap[i][j] * fadingAgent\n",
    "                    if tmpMap[i][j]>0.5:\n",
    "                        potTargets[0].append(j*50 + 24)\n",
    "                        potTargets[1].append(i*50 + 24)\n",
    "            # overlay the heatmap\n",
    "            heatmap = sns.heatmap(newProbaMap, ax=ax, cmap=cmap, linewidths=0.0)\n",
    "            heatmap.collections[0].set_alpha(0.35)\n",
    "            # mark the potential positions for the pools in the area\n",
    "            plt.scatter(potTargets[0], potTargets[1], marker='x', s=40, c='red')\n",
    "            # discard the x and y labels\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            # save the new heatmap\n",
    "            plt.savefig(\"./predictions/images/heatmaps/heatmap_{}\".format(self.removePrefix(img)), dpi=1000)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars\n",
    "satWidth, satHeight = 1600, 800\n",
    "imgWidth, imgHeight = 50, 50\n",
    "channels = 3\n",
    "batchSize = 16\n",
    "\n",
    "# load the model: PoolNet from the h5 file\n",
    "path2Weights = \"./predictions/weights/best_weights_baseline_3.h5\"\n",
    "trainFile = \"Train.py\"\n",
    "\n",
    "# check if the weights exist\n",
    "# load the model if the file exist otherwise train the model\n",
    "# using the trainHelper method implemented in `Train.py`\n",
    "if not os.path.isfile(path2Weights):\n",
    "    print(\"---Training The Model---\")\n",
    "    os.system(\"python {}\".format(trainFile))\n",
    "    print(\"---Model Trained---\")\n",
    "\n",
    "print(\"---Loading the Model---\")\n",
    "model = load_model(path2Weights)\n",
    "print(\"---Model Loaded---\")\n",
    "\n",
    "# Load the satellite images\n",
    "zonesPath = \"./data/zones/\"\n",
    "\n",
    "detect = Detector(model, zonesPath)\n",
    "\n",
    "tic = time.clock()\n",
    "probas, probaMap = detect.predictProba()\n",
    "results = detect.cleanProba(probas)\n",
    "elapsedTime = time.clock() - tic\n",
    "\n",
    "# detect.drawBoxes()\n",
    "detect.drawHeatmap(probaMap)\n",
    "print(\"Elapsed Time [s]: {:.3f}\".format(elapsedTime))\n",
    "# with open(\"predictions/results.json\",\"w\") as f:\n",
    "#     json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"END\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}